# BMPrinciples
A constantly updated collection of phenomenons observed during the scaling of big foundation models, which may be developed into consensus, principles, or laws in the future

We categorize these phenomena into different aspects during scaling.


1. Scaling of Computation
   1. Computational Optimal Language Model
   
2. Unifying Architecture
   1. Decoder-only wins in ICL
   
3. Optimal Hyperparameters
   1. Optimal Batchsize 
   2. Larger Batchsize Allows Better LR
   3. Optimal Scheduler (Ours)

4. Stable Training (?)
   1. Deep Norm
   
5. Training Dynamics
   1. Double Descent
   2. Grokking
   
6. Curriculum of Data
   1. General-then-Specific (Ours)

7. Task Capability
   1. Emergence
   2. Predictability
   3. Code-to-Reasoning?
   
8. Emergent Adaptation Method
   1. Prompt
   2. Delta
   3. ICL
   4. COT
   
9. Increased Sparsity
   1. Neuron-sparsity
   2. Delta-sparsity

10. Scaling of Alignment
    1.  Scaling Reward Model
    2.  Increased Calibration
    3.  Harder Interpretability
